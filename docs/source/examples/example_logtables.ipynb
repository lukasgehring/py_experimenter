{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Usage of Logtables\n",
    "\n",
    "This example shows the usage of `Logtables`. We will show how one can define and fill logtables. For this example you should already understand `PyExerimenter`s basic functionalities. Note that the this notebook does have limited amount of reasonable code.\n",
    "\n",
    "To execute this notebook you need to install:\n",
    "```\n",
    "pip install py_experimenter\n",
    "pip install scikit-learn\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment Configuration File\n",
    "This notebook shows an example execution of `PyExperimenter` based on the configuration file that is also use in the [general usage](https://tornede.github.io/py_experimenter/examples/example_general_usage.html) notebook. However this file is slightly adapted to show the usage of logtables. The goal in this small example is to find the best kernel for some dataset and log the performance of different kernels. To that end we show two ways of defining logtables: Standard Notation\n",
    "\n",
    "`logtables = train_scores:log_train_scores... `  \n",
    "`log_train_scores = f1:DOUBLE, accuracy:DOUBLE, kernel:str`\n",
    "\n",
    "and Shorthand notation\n",
    "\n",
    "`logtables = ..., test_f1:DOUBLE, test_accuracy:DOUBLE` .\n",
    "\n",
    "Instead of creating just the table `logtable_example`, this file specifies 3 more tables:\n",
    "\n",
    "`logtable_example__train_scores`,  \n",
    "`logtable_example__test_f1`,  \n",
    "`logtable_example__test_accuracy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "content = \"\"\"\n",
    "[PY_EXPERIMENTER]\n",
    "provider = sqlite \n",
    "database = automl_conf_2023\n",
    "table = logtable_example\n",
    "\n",
    "keyfields = dataset, cross_validation_splits:int, seed:int\n",
    "dataset = iris\n",
    "cross_validation_splits = 5\n",
    "seed = 1,2,3,4,5\n",
    "logtables = train_scores:log_train_scores, test_f1:DOUBLE, test_accuracy:DOUBLE \n",
    "log_train_scores = f1:DOUBLE, accuracy:DOUBLE, kernel:str\n",
    "\n",
    "resultfields = best_kernel_f1:VARCHAR(50), best_kernel_accuracy:VARCHAR(50)\n",
    "resultfields.timestamps = false\n",
    "\n",
    "[CUSTOM] \n",
    "path = sample_data\n",
    "\"\"\"\n",
    "\n",
    "# Create config directory if it does not exist\n",
    "if not os.path.exists('config'):\n",
    "    os.mkdir('config')\n",
    "    \n",
    "# Create config file\n",
    "experiment_configuration_file_path = os.path.join('config', 'example_general_usage.cfg')\n",
    "with open(experiment_configuration_file_path, \"w\") as f: \n",
    "  f.write(content)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the execution function\n",
    "Next, the execution of a single experiment has to be defined. Note that this dummy example is a slightly modified version of the [general usage](https://tornede.github.io/py_experimenter/examples/example_general_usage.html) notebook. Instead of executing with one kernel we iterate over kernels to find the best one. Additionally the resutls get logged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "from py_experimenter.result_processor import ResultProcessor\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "def run_ml(parameters: dict, result_processor: ResultProcessor, custom_config: dict):\n",
    "    seed = parameters['seed']\n",
    "\n",
    "    # Initalize variables\n",
    "    performance_f1 = 0\n",
    "    best_kernel_f1 = ''\n",
    "    performance_accuracy = 0\n",
    "    best_kernel_accuracy = ''\n",
    "    for kernel in ['linear', 'poly', 'rbf', 'sigmoid']:\n",
    "        # Set seed for reproducibility\n",
    "        random.seed(seed)\n",
    "        np.random.seed(seed)\n",
    "\n",
    "        data = load_iris()\n",
    "\n",
    "        X = data.data\n",
    "        y = data.target\n",
    "\n",
    "        model = make_pipeline(StandardScaler(), SVC(kernel=kernel, gamma='auto'))\n",
    "        scores = cross_validate(model, X, y,\n",
    "                                cv=parameters['cross_validation_splits'],\n",
    "                                scoring=('accuracy', 'f1_micro'),\n",
    "                                return_train_score=True\n",
    "                                )\n",
    "\n",
    "        # Log scores to logtables\n",
    "        result_processor.process_logs(\n",
    "            {\n",
    "                'train_scores': {\n",
    "                    'f1': np.mean(scores['train_f1_micro']),\n",
    "                    'accuracy': np.mean(scores['train_accuracy']),\n",
    "                    'kernel': \"'\" + kernel + \"'\"\n",
    "                },\n",
    "                'test_f1': {\n",
    "                    'test_f1': np.mean(scores['test_f1_micro'])},\n",
    "                'test_accuracy': {\n",
    "                    'test_accuracy': np.mean(scores['test_accuracy'])},\n",
    "            }\n",
    "        )\n",
    "\n",
    "        if np.mean(scores['test_f1_micro']) > performance_f1:\n",
    "            performance_f1 = np.mean(scores['test_f1_micro'])\n",
    "            best_kernel_f1 = kernel\n",
    "        if np.mean(scores['test_accuracy']) > performance_accuracy:\n",
    "            performance_accuracy = np.mean(scores['test_accuracy'])\n",
    "            best_kernel_accuracy = kernel\n",
    "\n",
    "    result_processor.process_results({\n",
    "        'best_kernel_f1': best_kernel_f1,\n",
    "        'best_kernel_accuracy': best_kernel_accuracy\n",
    "    })"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Executing PyExperimenter\n",
    "Now we create a PyExperimenter object with the config above as configuration file. We also fill the database with with values from the config file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from py_experimenter.experimenter import PyExperimenter\n",
    "\n",
    "experimenter = PyExperimenter(experiment_configuration_file_path=experiment_configuration_file_path, name='example_notebook')\n",
    "experimenter.fill_table_from_config()\n",
    "\n",
    "experimenter.get_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read one of the logtables\n",
    "experimenter.get_logtable('train_scores')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Experiments\n",
    "\n",
    "We then use the experimenter to execute the `run_ml` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experimenter.execute(run_ml, -1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check results\n",
    "Lastly the content of all tables is returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experimenter.get_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experimenter.get_logtable('train_scores')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experimenter.get_logtable('test_f1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experimenter.get_logtable('test_accuracy')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py-experimenter",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "771bc47fe5ac54f21af9466a0ce1d237446274d8107817e4b77b2018f606f4bf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
